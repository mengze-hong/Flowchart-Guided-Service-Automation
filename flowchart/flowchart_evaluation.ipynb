{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import List, Dict, Optional\n",
    "import networkx as nx\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05538cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowchartEvaluator:\n",
    "    def __init__(self, api_key: str):\n",
    "        # Initialize OpenAI API\n",
    "        openai.api_key = api_key\n",
    "        openai.api_base = \"https://api.xty.app/v1\"\n",
    "        self.model = \"gpt-4\"\n",
    "        self.temperature = 0\n",
    "        self.request_timeout = 10\n",
    "\n",
    "    def parse_mermaid_flowchart(self, mermaid_text: str) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Parse Mermaid flowchart text to extract nodes with descriptions and their successors.\n",
    "        Returns a list of tuples: (node_id, description, list_of_successors)\n",
    "        Successors are sorted alphabetically.\n",
    "        \"\"\"\n",
    "        # Extract nodes and descriptions from original text\n",
    "        node_rect_pattern = r'(\\w+)\\[\"([^\"]*)\"\\]'\n",
    "        node_dec_pattern = r'(\\w+)\\{([^}]*)\\}'\n",
    "        \n",
    "        nodes = {}\n",
    "        \n",
    "        # Parse rectangular nodes\n",
    "        for match in re.finditer(node_rect_pattern, mermaid_text):\n",
    "            node_id = match.group(1)\n",
    "            desc = match.group(2).strip()\n",
    "            nodes[node_id] = desc\n",
    "        \n",
    "        # Parse decision nodes\n",
    "        for match in re.finditer(node_dec_pattern, mermaid_text):\n",
    "            node_id = match.group(1)\n",
    "            desc = match.group(2).strip()\n",
    "            nodes[node_id] = desc\n",
    "        \n",
    "        # Normalize text for edge parsing: replace node definitions with IDs\n",
    "        normalized = re.sub(node_rect_pattern, r'\\1', mermaid_text)\n",
    "        normalized = re.sub(node_dec_pattern, r'\\1', normalized)\n",
    "        \n",
    "        # Normalize all whitespace to single spaces\n",
    "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "        \n",
    "        # Edge pattern: source --> [|cond|] target\n",
    "        edge_pattern = r'(\\w+)\\s*-->\\s*(?:\\|([^|]*)\\|\\s*)?(\\w+)'\n",
    "        \n",
    "        edges = []\n",
    "        for match in re.finditer(edge_pattern, normalized):\n",
    "            source = match.group(1)\n",
    "            condition = match.group(2)  # May be None if no condition\n",
    "            target = match.group(3)\n",
    "            edges.append((source, target))\n",
    "        \n",
    "        # Build adjacency list\n",
    "        adj = {node_id: [] for node_id in nodes}\n",
    "        for source, target in edges:\n",
    "            if source in adj and target in adj:\n",
    "                if target not in adj[source]:\n",
    "                    adj[source].append(target)\n",
    "        \n",
    "        # Sort successors\n",
    "        for node_id in adj:\n",
    "            adj[node_id].sort()\n",
    "        \n",
    "        # Return sorted list of tuples\n",
    "        result = [(node_id, nodes[node_id], adj[node_id]) for node_id in sorted(nodes.keys())]\n",
    "        \n",
    "        # Print parsed nodes\n",
    "        # print(\"Parsed Flowchart Nodes:\")\n",
    "        # for node_id, desc, successors in result:\n",
    "        #     node_type = self._extract_node_type(desc)\n",
    "        #     print(f\"Node ID: {node_id}, Type: {node_type}, Description: {desc}, Successors: {successors}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _extract_node_type(self, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract node type from description (e.g., \"Start:\", \"Decision:\", \"End:\").\n",
    "        \"\"\"\n",
    "        if ':' in description:\n",
    "            prefix = description.split(':')[0].strip().lower()\n",
    "            type_map = {\n",
    "                'start': 'start',\n",
    "                'prompt': 'prompt',\n",
    "                'decision': 'decision',\n",
    "                'action': 'action',\n",
    "                'output': 'output',\n",
    "                'reflection': 'reflection',\n",
    "                'end': 'end'\n",
    "            }\n",
    "            return type_map.get(prefix, prefix)\n",
    "        return description.lower()\n",
    "\n",
    "    def call_gpt4o_mini(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Call gpt-4 to process a prompt for node matching.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                api_base=\"https://api.xty.app/v1\",\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                request_timeout=self.request_timeout\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling gpt-4: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def match_utterance_to_node(self, utterance: str, node_descriptions: Dict[str, str]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Prompt gpt-4 to identify which node the utterance corresponds to.\n",
    "        Returns node ID or None if no match is found.\n",
    "        \"\"\"\n",
    "        if not node_descriptions:\n",
    "            return None\n",
    "        node_list = \"\\n\".join([f\"Node ID: {node_id}, Description: {desc}\" for node_id, desc in node_descriptions.items()])\n",
    "        prompt = (\n",
    "            f\"Given the following utterance and flowchart node descriptions, identify which node the utterance corresponds to based on the semantic.\\n\\n\"\n",
    "            f\"When the utterance express query or task (e.g., I am looking for), it corresponds to the start node A. When the utterance express appreciation, it means the end of the conversation.\\n\\n\"\n",
    "            f\"Return only the Node ID if a match is found, or 'None' if no node matches.\\n\\n\"\n",
    "            f\"Utterance: {utterance}\\n\\n\"\n",
    "            f\"Node Descriptions:\\n{node_list}\\n\\n\"\n",
    "            f\"Output only the Node ID or 'None'.\"\n",
    "        )\n",
    "        response = self.call_gpt4o_mini(prompt)\n",
    "        return response if response != \"None\" else None\n",
    "    \n",
    "    def is_complete_dialogue(self, node_sequence: List[str], end_node: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the dialogue is complete: the end node is the last node or there are no None nodes after it.\n",
    "        \"\"\"\n",
    "        # print(node_sequence)\n",
    "        if end_node not in node_sequence:\n",
    "            return False\n",
    "        end_index = node_sequence.index(end_node)\n",
    "        # Complete if end_node is the last node or no None values appear after it\n",
    "        return end_index == len(node_sequence) - 1 or None not in node_sequence[end_index + 1:]\n",
    "\n",
    "    def compute_cpc(self, classification_results: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Compute Complete Path Coverage (CPC) from classification results.\n",
    "        \"\"\"\n",
    "        if not classification_results:\n",
    "            return 0.0\n",
    "        complete_paths = sum(1 for result in classification_results if result[\"has_complete_path\"])\n",
    "        total_dialogues = len(classification_results)\n",
    "        return complete_paths / total_dialogues if total_dialogues > 0 else 0.0\n",
    "\n",
    "    def compute_umr(self, node_sequence: List[tuple]) -> float:\n",
    "        \"\"\"\n",
    "        Compute Utterance Matching Ratio (UMR) for a single dialogue from node sequence.\n",
    "        \"\"\"\n",
    "        if not node_sequence:\n",
    "            return 0.0\n",
    "        matched_utterances = sum(1 for _, node_id in node_sequence if node_id is not None)\n",
    "        return matched_utterances / len(node_sequence)\n",
    "\n",
    "    def compute_avg_umr(self, classification_results: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Compute average UMR across all dialogues from classification results.\n",
    "        \"\"\"\n",
    "        if not classification_results:\n",
    "            return 0.0\n",
    "        umr_scores = [self.compute_umr(result[\"utterance_node_pairs\"]) for result in classification_results]\n",
    "        return sum(umr_scores) / len(umr_scores) if umr_scores else 0.0\n",
    "\n",
    "    def evaluate(self, mermaid_flowchart: str, dialogues: List[List[str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate the flowchart against the dialogue dataset and return CPC, average UMR, and classification results.\n",
    "        \"\"\"\n",
    "        # Parse Mermaid flowchart\n",
    "        parsed_flowchart = self.parse_mermaid_flowchart(mermaid_flowchart)\n",
    "        if not parsed_flowchart:\n",
    "            print(\"Error: No nodes parsed from flowchart. Verify Mermaid syntax.\")\n",
    "            return {\n",
    "                \"CPC\": 0.0,\n",
    "                \"Average_UMR\": 0.0,\n",
    "                \"Classification_Results\": [],\n",
    "                \"Error\": \"No nodes parsed from flowchart. Verify Mermaid syntax.\"\n",
    "            }\n",
    "\n",
    "        # Build node descriptions dictionary\n",
    "        node_descriptions = {node_id: desc for node_id, desc, _ in parsed_flowchart}\n",
    "\n",
    "        # Find end node, with fallback\n",
    "        try:\n",
    "            end_node = next(\n",
    "                node_id for node_id, desc, _ in parsed_flowchart\n",
    "                if self._extract_node_type(desc) == \"end\"\n",
    "            )\n",
    "            print(f\"End Node Identified: {end_node}\")\n",
    "        except StopIteration:\n",
    "            print(\"Error: No node with type 'end' found in the flowchart.\")\n",
    "            return {\n",
    "                \"CPC\": 0.0,\n",
    "                \"Average_UMR\": 0.0,\n",
    "                \"Classification_Results\": [],\n",
    "                \"Error\": \"No end node found in flowchart\"\n",
    "            }\n",
    "\n",
    "        # Compute node sequences and completeness with progress bar\n",
    "        classification_results = []\n",
    "        print(\"\\nEvaluating Dialogues:\")\n",
    "        for i, dialogue in tqdm(enumerate(dialogues), total=len(dialogues), desc=\"Processing Dialogues\"):\n",
    "            print(f\"\\nDialogue {i}:\")\n",
    "            node_sequence = []\n",
    "            for j, utt in enumerate(tqdm(dialogue, desc=f\"Matching Utterances for Dialogue {i}\", leave=False)):\n",
    "                node_id = self.match_utterance_to_node(utt, node_descriptions)\n",
    "                node_sequence.append((j, node_id))\n",
    "                print(f\"  Utterance {j}: '{utt}' -> Node {node_id if node_id else 'None'}\")\n",
    "            \n",
    "            # Filter out None values for completeness check\n",
    "            nodes = [node for _, node in node_sequence]\n",
    "            has_complete_path = self.is_complete_dialogue(nodes, end_node)\n",
    "            print(f\"  Complete: {has_complete_path}\")\n",
    "            classification_results.append({\n",
    "                \"dialogue_id\": i,\n",
    "                \"has_complete_path\": has_complete_path,\n",
    "                \"utterance_node_pairs\": node_sequence\n",
    "            })\n",
    "\n",
    "        # Compute metrics from classification results\n",
    "        cpc = self.compute_cpc(classification_results)\n",
    "        avg_umr = self.compute_avg_umr(classification_results)\n",
    "\n",
    "        return {\n",
    "            \"CPC\": cpc,\n",
    "            \"Average_UMR\": avg_umr,\n",
    "            \"Classification_Results\": classification_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38126fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Move one directory up from current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# data = load_data()\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = ['hotel', 'restaurant', 'attraction', 'train', 'taxi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dialog_text(dialog):\n",
    "    utterances = []\n",
    "    for i, turn in enumerate(dialog['log']):\n",
    "        role = 'User' if i % 2 == 0 else 'AI Assistant'\n",
    "        utterance = [turn[\"text\"]]\n",
    "        if role == 'User':\n",
    "            utterances.append(utterance)\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = {}\n",
    "\n",
    "# for domain in DOMAINS:\n",
    "#     print(f\"--- Domain: {domain} ---\")\n",
    "#     dialogues = []\n",
    "#     ids = []\n",
    "#     for i in range(0, 50):\n",
    "#         dialog, dialog_id = pick_dialog(data, dialog_id='random', domain=domain, exclusive=False)\n",
    "#         dialogue_text = parse_dialog_text(dialog)\n",
    "#         dialogues.append(dialogue_text)\n",
    "#         ids.append(dialog_id)\n",
    "#     test_data[domain] = dialogues\n",
    "\n",
    "#     with open(f\"flowchart_eval/dialogues_{domain}_{current_time}.json\", \"w\") as f:\n",
    "#         json.dump({\"ids\": ids}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "DOMAINS = ['Train', 'Hotel', 'Restaurant', 'Attraction', 'Taxi']\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Load JSON files for each domain\n",
    "loaded_ids = {}\n",
    "for domain in DOMAINS:\n",
    "    filename = f\"flowchart_eval/dialogues_{domain}_2025-09-19-16-07.json\"\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            loaded_ids[domain.lower()] = data[\"ids\"]\n",
    "        print(f\"Loaded {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from {filename}\")\n",
    "\n",
    "# Load dialogue data\n",
    "data = load_data()\n",
    "print(f\"Total dialogues loaded: {len(data)}\")\n",
    "\n",
    "# Parse dialogues and organize by domain\n",
    "output_data = {\"test_dialogue_ids\": {}, \"test_dialogue_texts\": {}}\n",
    "for domain, ids in loaded_ids.items():\n",
    "    dialogue_ids = []\n",
    "    dialogue_texts = []\n",
    "    print(f\"--- Domain: {domain} ---\")\n",
    "    for dialog_id in ids:\n",
    "        try:\n",
    "            dialog, selected_id = pick_dialog(data, dialog_id=dialog_id, domain=domain, exclusive=False)\n",
    "            dialogue_text = parse_dialog_text(dialog)\n",
    "            dialogue_ids.append(selected_id)\n",
    "            dialogue_texts.append(dialogue_text)\n",
    "            print(f\"Dialog ID {selected_id} in {domain}: {dialogue_text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error picking dialog ID {dialog_id} in {domain}: {str(e)}\")\n",
    "    \n",
    "    output_data[\"test_dialogue_ids\"][domain] = dialogue_ids\n",
    "    output_data[\"test_dialogue_texts\"][domain] = dialogue_texts\n",
    "\n",
    "# Save to a single file\n",
    "output_filename = f\"flowchart_eval/dialogues_all.json\"\n",
    "try:\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "    print(f\"Saved dialogues to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to {output_filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the single dialogue file\n",
    "filename = f\"flowchart_eval/dialogues_all.json\"\n",
    "test_data = {}\n",
    "\n",
    "try:\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for domain in DOMAINS:\n",
    "            domain_lower = domain.lower()\n",
    "            test_data[domain.lower()] = data[\"test_dialogue_texts\"][domain.lower()]\n",
    "    print(f\"Loaded dialogues from {filename}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {filename} not found\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON from {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6160057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Input Mermaid flowchart\n",
    "with open(\"selected_flowchart_merged.mermaid\", \"r\") as f:\n",
    "    mermaid_flowchart = f.read()\n",
    "\n",
    "# Parse the flowchart to extract domains\n",
    "def parse_mermaid_flowchart(flowchart):\n",
    "    domains = {}\n",
    "    current_domain = None\n",
    "    domain_lines = []\n",
    "    in_subgraph = False\n",
    "\n",
    "    for line in flowchart.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Detect start of a subgraph\n",
    "        if line.startswith('subgraph'):\n",
    "            current_domain = re.search(r'subgraph\\s+(\\w+)\\[\"([^\"]+)\"\\]', line)\n",
    "            if current_domain:\n",
    "                current_domain = current_domain.group(1)  # e.g., D_T, D_H\n",
    "                domain_lines = []\n",
    "                in_subgraph = True\n",
    "            continue\n",
    "\n",
    "        # Detect end of a subgraph\n",
    "        if line == 'end' and in_subgraph:\n",
    "            # Add an explicit end node to the domain lines\n",
    "            last_node = domain_lines[-1].split('-->')[-1].strip().split('[')[0].strip()\n",
    "            domain_lines.append(f\"{last_node} --> {current_domain}_END[\\\"End: Goodbye\\\"]\")\n",
    "            domains[current_domain] = domain_lines\n",
    "            in_subgraph = False\n",
    "            current_domain = None\n",
    "            continue\n",
    "\n",
    "        # Collect lines within a subgraph\n",
    "        if in_subgraph:\n",
    "            domain_lines.append(line)\n",
    "\n",
    "    return domains\n",
    "\n",
    "# Generate individual Mermaid files for each domain\n",
    "def generate_domain_files(domains):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"./flowcharts_generated\", exist_ok=True)\n",
    "\n",
    "    for domain_id, lines in domains.items():\n",
    "        domain_name = {\n",
    "            'D_T': 'Train',\n",
    "            'D_H': 'Hotel',\n",
    "            'D_R': 'Restaurant',\n",
    "            'D_A': 'Attraction',\n",
    "            'D_M': 'Multi',\n",
    "            'D_X': 'Taxi'\n",
    "        }.get(domain_id, domain_id)\n",
    "\n",
    "        # Create file content\n",
    "        file_content = \"flowchart TD\\n\"\n",
    "        file_content += '\\n'.join(f\"    {line}\" for line in lines)\n",
    "\n",
    "        # Write to file\n",
    "        filename = f\"./flowcharts_generated/{domain_name.lower()}.workflow\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(file_content)\n",
    "        print(f\"Generated {filename}\")\n",
    "\n",
    "# Main execution\n",
    "domains = parse_mermaid_flowchart(mermaid_flowchart)\n",
    "generate_domain_files(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "def load_flowcharts(scenarios_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load flowchart data from .workflow files in the specified directory.\n",
    "    Returns a dictionary mapping domain names to flowchart text.\n",
    "    \n",
    "    Args:\n",
    "        scenarios_dir (str): Directory containing the .workflow files.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary with domain names as keys and flowchart text as values.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store flowcharts\n",
    "    flowcharts = {}\n",
    "    \n",
    "    # Define domains and corresponding flowchart files\n",
    "    domains = [\"restaurant\", \"hotel\", \"attraction\", \"train\", \"taxi\"]\n",
    "    \n",
    "    try:\n",
    "        # Read flowchart files for each domain\n",
    "        for domain in domains:\n",
    "            file_path = os.path.join(scenarios_dir, f\"{domain}.workflow\")\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                flowcharts[domain] = f.read()\n",
    "        print(\"Successfully read all flowchart files.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "    \n",
    "    return flowcharts\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# scenarios_dir = \"./prompts/scenarios/\"\n",
    "scenarios_dir = \"./abstraction_flowcharts_generated\"\n",
    "flowcharts = load_flowcharts(scenarios_dir)\n",
    "\n",
    "    # Print loaded flowcharts\n",
    "    # for domain, flowchart in flowcharts.items():\n",
    "    #     print(f\"\\nDomain: {domain}\")\n",
    "    #     print(f\"Flowchart:\\n{flowchart[:100]}...\")  # Print first 100 chars for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming DOMAINS, test_data, and flowcharts are defined\n",
    "DOMAINS = [\"restaurant\", \"hotel\", \"attraction\", \"train\", \"taxi\"]\n",
    "\n",
    "# Initialize dictionary to store results for all domains\n",
    "all_results = {}\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    print(f\"\\n=== Evaluating Domain: {domain} ===\")\n",
    "    # Use domain-specific dialogues from test_data\n",
    "    dialogues = test_data[domain]\n",
    "    # Get the domain-specific flowchart\n",
    "    mermaid_flowchart = flowcharts[domain]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = FlowchartEvaluator(api_key=\"sk-xxxxx\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate(mermaid_flowchart, dialogues)\n",
    "    \n",
    "    # Print results with domain prefix\n",
    "    if \"Error\" not in results:\n",
    "        print(f\"{domain} - Complete Path Coverage (CPC): {results['CPC']:.4f}\")\n",
    "        print(f\"{domain} - Average Utterance Matching Ratio (UMR): {results['Average_UMR']:.4f}\")\n",
    "\n",
    "        # print(f\"{domain} - Classification Results:\")\n",
    "        # for result in results['Classification_Results']:\n",
    "        #     print(f\"Dialogue {result['dialogue_id']}: {'Complete' if result['has_complete_path'] else 'Incomplete'}\")\n",
    "        #     print(\"Utterance-Node Pairs:\")\n",
    "        #     for utt_id, node_id in result['utterance_node_pairs']:\n",
    "        #         print(f\"  Utterance {utt_id}: Node {node_id if node_id else 'None'}\")\n",
    "    else:\n",
    "        print(f\"{domain} - Error: {results['Error']}\")\n",
    "    \n",
    "    # Store results for this domain\n",
    "    all_results[domain] = results\n",
    "\n",
    "# Save all results to a single JSON file\n",
    "with open(f\"flowchart_evaluation_abstraction_new.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(\"\\nResults saved to flowchart_evaluation_abstraction.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
